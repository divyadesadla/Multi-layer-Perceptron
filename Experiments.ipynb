{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mlp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = np.load('../data/trainX.npy')\n",
    "train_output = np.load('../data/trainY.npy')\n",
    "\n",
    "test_input = np.load('../data/testX.npy')\n",
    "test_output = np.load('../data/testY.npy')\n",
    "\n",
    "train_mean = np.mean(train_input,axis=0)\n",
    "train_std = np.std(train_input,axis=0)\n",
    "\n",
    "\n",
    "train_input = (train_input - train_mean)/(train_std + 1e-16)\n",
    "test_input = (test_input - train_mean)/(train_std + 1e-16)\n",
    "indim = train_input[0].shape[0]\n",
    "print(indim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label1 = train_output[:,0]\n",
    "train_label2 = train_output[:,1]\n",
    "\n",
    "test_label1 = test_output[:,0]\n",
    "test_label2 = test_output[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_normal_weight_init(input, output):\n",
    "    return np.random.normal(0,1,(output,input))\n",
    "\n",
    "def random_weight_init(input,output):\n",
    "    b = np.sqrt(6)/np.sqrt(input+output)\n",
    "    return np.random.uniform(-b,b,(output,input))\n",
    "\n",
    "def zeros_bias_init(outd):\n",
    "    return np.zeros((outd,1))\n",
    "\n",
    "def labels2onehot(labels):\n",
    "    return np.array([[i==lab for i in range(10)]for lab in labels])\n",
    "\n",
    "\n",
    "train_label1_onehot = labels2onehot(train_label1)\n",
    "train_label2_onehot = labels2onehot(train_label2)\n",
    "\n",
    "test_label1_onehot = labels2onehot(test_label1)\n",
    "test_label2_onehot = labels2onehot(test_label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(input_data,label1_onehot,label2_onehot,batch_size):\n",
    "    input_batches = []\n",
    "    output1_batches = []\n",
    "    output2_batches = []\n",
    "    indices = np.arange(len(input_data))\n",
    "   \n",
    "    \n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for i in range(int(len(indices)/batch_size)):\n",
    "            input_data_batch = input_data[i*batch_size:(i+1)*batch_size]\n",
    "            batch_label1_onehot = label1_onehot[i*batch_size:(i+1)*batch_size]\n",
    "            batch_label2_onehot = label2_onehot[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "\n",
    "            input_batches.append(input_data_batch)\n",
    "            output1_batches.append(batch_label1_onehot)\n",
    "            output2_batches.append(batch_label2_onehot)\n",
    "\n",
    "        return input_batches, output1_batches, output2_batches\n",
    "\n",
    "\n",
    "input_batches, output1_batches, output2_batches = create_batches(train_input,train_label1_onehot,train_label2_onehot,32)\n",
    "test_input_batches,test_output1_batches,test_output2_batches = create_batches(test_input,test_label1_onehot,test_label2_onehot,32)\n",
    "\n",
    "input_batches = np.array(input_batches)\n",
    "output1_batches = np.array(output1_batches)\n",
    "output2_batches = np.array(output2_batches)\n",
    "\n",
    "test_input_batches = np.array(test_input_batches)\n",
    "test_output1_batches = np.array(test_output1_batches)\n",
    "test_output2_batches = np.array(test_output2_batches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SingleLayerSingleTaskMLP = mlp.SingleLayerSingleTaskMLP(inp=indim,outp=10,\n",
    "                                                        hiddenlayer=100,alpha=0,dropout_chance=0,lr=0.001)\n",
    "\n",
    "\n",
    "TwoLayerSingleTaskMLP = mlp.TwoLayerSingleTaskMLP(inp=indim,outp=10,\n",
    "                                                  hiddenlayers=[100,100],alpha=0,dropout_chance=0,lr=0.001)\n",
    "\n",
    "\n",
    "TwoLayerTwoTaskMLP = mlp.TwoLayerTwoTaskMLP(inp=indim,outp=10,\n",
    "                                            hiddenlayers=[100,100],alpha=0,dropout_chance=0,lr=0.001) \n",
    "\n",
    "\n",
    "\n",
    "SingleLayerSingleTaskMLP.loadparams([random_weight_init(indim,SingleLayerSingleTaskMLP.hiddenlayer),random_weight_init(SingleLayerSingleTaskMLP.hiddenlayer,output1_batches[0].shape[1])]\n",
    "                                    ,[zeros_bias_init(SingleLayerSingleTaskMLP.hiddenlayer),zeros_bias_init(output1_batches[0].shape[1])])\n",
    "\n",
    "\n",
    "\n",
    "TwoLayerSingleTaskMLP.loadparams([random_weight_init(indim,TwoLayerSingleTaskMLP.hiddenlayers[0]),random_weight_init(TwoLayerSingleTaskMLP.hiddenlayers[0],TwoLayerSingleTaskMLP.hiddenlayers[1]),random_weight_init(TwoLayerSingleTaskMLP.hiddenlayers[1],output1_batches[0].shape[1])]\n",
    "                                    ,[zeros_bias_init(TwoLayerSingleTaskMLP.hiddenlayers[0]),zeros_bias_init(TwoLayerSingleTaskMLP.hiddenlayers[1]),zeros_bias_init(output1_batches[0].shape[1])])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TwoLayerTwoTaskMLP.loadparams([random_weight_init(indim,TwoLayerTwoTaskMLP.hiddenlayers[0]),random_weight_init(TwoLayerTwoTaskMLP.hiddenlayers[0],TwoLayerTwoTaskMLP.hiddenlayers[1]),random_weight_init(TwoLayerTwoTaskMLP.hiddenlayers[1],output1_batches[0].shape[1]),random_weight_init(TwoLayerTwoTaskMLP.hiddenlayers[1],output1_batches[0].shape[1])]\n",
    "                                    ,[zeros_bias_init(TwoLayerTwoTaskMLP.hiddenlayers[0]),zeros_bias_init(TwoLayerTwoTaskMLP.hiddenlayers[1]),zeros_bias_init(output1_batches[0].shape[1]),zeros_bias_init(output1_batches[0].shape[1])])\n",
    "\n",
    "\n",
    "SoftmaxCrossEntropyLoss = mlp.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "SoftmaxCrossEntropyLoss_two_task1 = mlp.SoftmaxCrossEntropyLoss()\n",
    "SoftmaxCrossEntropyLoss_two_task2 = mlp.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_train_nn(train_data,test_data,train_label1_onehot,test_label1_onehot,train_x,train_y,test_x,test_y,learning_rate,momentum,dropout_rate,epochs,batch_size):\n",
    "    single_layer_train_loss_array = []\n",
    "    single_layer_train_accuracy_array = []\n",
    "    single_layer_test_loss_array = []\n",
    "    single_layer_test_accuracy_array = []\n",
    "    \n",
    "    for j in range(epochs):\n",
    "        for i in range(len(train_x)):\n",
    "            train_a = SingleLayerSingleTaskMLP.forward(train_x[i].T)\n",
    "            single_layer_loss_train = SoftmaxCrossEntropyLoss.forward(train_a,train_y[i].T)\n",
    "            train_c = SoftmaxCrossEntropyLoss.backward()\n",
    "            train_d = SingleLayerSingleTaskMLP.backward(train_c)\n",
    "            train_e = SingleLayerSingleTaskMLP.step()\n",
    "            train_f = SingleLayerSingleTaskMLP.zerograd()\n",
    "            \n",
    "        train_a_data = SingleLayerSingleTaskMLP.forward(train_data.T)\n",
    "        single_layer_loss_train = SoftmaxCrossEntropyLoss.forward(train_a_data,train_label1_onehot.T)\n",
    "        single_layer_accuracy_train = (np.argmax(train_a_data,axis=0)==np.argmax(train_label1_onehot.T, axis=0)).sum()/train_a_data.shape[1]\n",
    "        \n",
    "        test_a_data = SingleLayerSingleTaskMLP.forward(test_data.T)\n",
    "        single_layer_loss_test = SoftmaxCrossEntropyLoss.forward(test_a_data,test_label1_onehot.T)\n",
    "        single_layer_accuracy_test = (np.argmax(test_a_data,axis=0)==np.argmax(test_label1_onehot.T, axis=0)).sum()/test_a_data.shape[1]\n",
    "        \n",
    "        single_layer_train_loss_array.append(single_layer_loss_train)\n",
    "        single_layer_test_loss_array.append(single_layer_loss_test)\n",
    "        \n",
    "        single_layer_train_accuracy_array.append(single_layer_accuracy_train)\n",
    "        single_layer_test_accuracy_array.append(single_layer_accuracy_test)\n",
    "        \n",
    "        \n",
    "        print('epoch={}'.format(j+1),'train_loss={}'.format(single_layer_loss_train),'accuracy_train={}'.format(single_layer_accuracy_train))\n",
    "        print('epoch={}'.format(j+1),'test_loss={}'.format(single_layer_loss_test),'accuracy_test={}'.format(single_layer_accuracy_test))\n",
    "        print('.............................................................................................')\n",
    "    return single_layer_train_loss_array,single_layer_test_loss_array,single_layer_train_accuracy_array,single_layer_test_accuracy_array\n",
    "\n",
    "\n",
    "def two_layer_single_task_train_nn(train_data,test_data,train_label1_onehot,test_label1_onehot,train_x,train_y,test_x,test_y,learning_rate,momentum,dropout_rate,epochs):\n",
    "    two_layer_train_loss_array = []\n",
    "    two_layer_train_accuracy_array = []\n",
    "    two_layer_test_loss_array = []\n",
    "    two_layer_test_accuracy_array = []\n",
    "    \n",
    "    for j in range(epochs):\n",
    "        for i in range(len(train_x)):\n",
    "            train_a = TwoLayerSingleTaskMLP.forward(train_x[i].T)\n",
    "            two_layer_loss_train = SoftmaxCrossEntropyLoss.forward(train_a,train_y[i].T)\n",
    "            train_c = SoftmaxCrossEntropyLoss.backward()\n",
    "            train_d = TwoLayerSingleTaskMLP.backward(train_c)\n",
    "            train_e = TwoLayerSingleTaskMLP.step()\n",
    "            train_f = TwoLayerSingleTaskMLP.zerograd()\n",
    "            \n",
    "        train_a_data = TwoLayerSingleTaskMLP.forward(train_data.T)\n",
    "        two_layer_loss_train = SoftmaxCrossEntropyLoss.forward(train_a_data,train_label1_onehot.T)\n",
    "        two_layer_accuracy_train = (np.argmax(train_a_data,axis=0)==np.argmax(train_label1_onehot.T, axis=0)).sum()/train_a_data.shape[1]\n",
    "        \n",
    "        test_a_data = TwoLayerSingleTaskMLP.forward(test_data.T)\n",
    "        two_layer_loss_test = SoftmaxCrossEntropyLoss.forward(test_a_data,test_label1_onehot.T)\n",
    "        two_layer_accuracy_test = (np.argmax(test_a_data,axis=0)==np.argmax(test_label1_onehot.T, axis=0)).sum()/test_a_data.shape[1]\n",
    "        \n",
    "        two_layer_train_loss_array.append(two_layer_loss_train)\n",
    "        two_layer_test_loss_array.append(two_layer_loss_test)\n",
    "        \n",
    "        two_layer_train_accuracy_array.append(two_layer_accuracy_train)\n",
    "        two_layer_test_accuracy_array.append(two_layer_accuracy_test)\n",
    "        \n",
    "        \n",
    "        print('epoch={}'.format(j+1),'train_loss={}'.format(two_layer_loss_train),'accuracy_train={}'.format(two_layer_accuracy_train))\n",
    "        print('epoch={}'.format(j+1),'test_loss={}'.format(two_layer_loss_test),'accuracy_test={}'.format(two_layer_accuracy_test))\n",
    "        print('.............................................................................................')\n",
    "    return two_layer_train_loss_array, two_layer_test_loss_array, two_layer_train_accuracy_array, two_layer_test_accuracy_array\n",
    "\n",
    "def two_layer_two_task_train_nn(train_data,test_data,train_label1_onehot,train_label2_onehot,test_label1_onehot,test_label2_onehot,train_x,train_y1,train_y2,test_x,test_y1,test_y2,learning_rate,momentum,dropout_rate,epochs):\n",
    "    two_task_train_loss_array1 = []\n",
    "    two_task_train_loss_array2 = []\n",
    "    two_task_train_accuracy_array1 = []\n",
    "    two_task_train_accuracy_array2 = []\n",
    "    \n",
    "    two_task_test_loss_array1 = []\n",
    "    two_task_test_loss_array2 = []\n",
    "    two_task_test_accuracy_array1 = []\n",
    "    two_task_test_accuracy_array2 = []\n",
    "    \n",
    "    for j in range(epochs):\n",
    "        for i in range(len(train_x)):\n",
    "            train_a1,train_a2 = TwoLayerTwoTaskMLP.forward(train_x[i].T)\n",
    "            SoftmaxCrossEntropyLoss_two_task1.forward(train_a1,train_y1[i].T)\n",
    "            SoftmaxCrossEntropyLoss_two_task2.forward(train_a2,train_y2[i].T)\n",
    "            train_c1 = SoftmaxCrossEntropyLoss_two_task1.backward()\n",
    "            train_c2 = SoftmaxCrossEntropyLoss_two_task2.backward()\n",
    "            train_d = TwoLayerTwoTaskMLP.backward([train_c1,train_c2])\n",
    "            train_e = TwoLayerTwoTaskMLP.step()\n",
    "            train_f = TwoLayerTwoTaskMLP.zerograd()\n",
    "            \n",
    "        train_a_data1, train_a_data2 = TwoLayerTwoTaskMLP.forward(train_data.T)\n",
    "        two_layer_two_task_loss_train1 = SoftmaxCrossEntropyLoss_two_task1.forward(train_a_data1,train_label1_onehot.T)\n",
    "        two_layer_two_task_loss_train2 = SoftmaxCrossEntropyLoss_two_task2.forward(train_a_data2,train_label2_onehot.T)\n",
    "        two_layer_two_task_accuracy_train1 = (np.argmax(train_a_data1,axis=0)==np.argmax(train_label1_onehot.T, axis=0)).sum()/train_a_data1.shape[1]\n",
    "        two_layer_two_task_accuracy_train2 = (np.argmax(train_a_data2,axis=0)==np.argmax(train_label2_onehot.T, axis=0)).sum()/train_a_data2.shape[1]\n",
    "        \n",
    "        test_a_data1,test_a_data2 = TwoLayerTwoTaskMLP.forward(test_data.T)\n",
    "        two_layer_two_task_loss_test1 = SoftmaxCrossEntropyLoss_two_task1.forward(test_a_data1,test_label1_onehot.T)\n",
    "        two_layer_two_task_loss_test2 = SoftmaxCrossEntropyLoss_two_task2.forward(test_a_data2,test_label2_onehot.T)\n",
    "        two_layer_two_task_accuracy_test1 = (np.argmax(test_a_data1,axis=0)==np.argmax(test_label1_onehot.T, axis=0)).sum()/test_a_data1.shape[1]\n",
    "        two_layer_two_task_accuracy_test2 = (np.argmax(test_a_data2,axis=0)==np.argmax(test_label2_onehot.T, axis=0)).sum()/test_a_data2.shape[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        two_task_train_loss_array1.append(two_layer_two_task_loss_train1)\n",
    "        two_task_train_loss_array2.append(two_layer_two_task_loss_train2)\n",
    "        \n",
    "        two_task_test_loss_array1.append(two_layer_two_task_loss_test1)\n",
    "        two_task_test_loss_array2.append(two_layer_two_task_loss_test2)\n",
    "        \n",
    "        two_task_train_accuracy_array1.append(two_layer_two_task_accuracy_train1)\n",
    "        two_task_train_accuracy_array2.append(two_layer_two_task_accuracy_train2)\n",
    "        \n",
    "        two_task_test_accuracy_array1.append(two_layer_two_task_accuracy_test1)\n",
    "        two_task_test_accuracy_array2.append(two_layer_two_task_accuracy_test2)\n",
    "        \n",
    "        \n",
    "        print('epoch={}'.format(j+1),'train_loss={}'.format(two_layer_two_task_loss_train2),'accuracy_train={}'.format(two_layer_two_task_accuracy_train2))\n",
    "        print('epoch={}'.format(j+1),'test_loss={}'.format(two_layer_two_task_loss_test2),'accuracy_test={}'.format(two_layer_two_task_accuracy_test2))\n",
    "        print('.............................................................................................')\n",
    "    return two_task_train_loss_array1, two_task_train_loss_array2, two_task_test_loss_array1, two_task_test_loss_array2, two_task_train_accuracy_array1, two_task_train_accuracy_array2, two_task_test_accuracy_array1, two_task_test_accuracy_array2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "single_layer_train_loss_array,single_layer_test_loss_array,single_layer_train_accuracy_array,single_layer_test_accuracy_array = single_layer_train_nn(train_input,test_input,train_label1_onehot,test_label1_onehot,input_batches,output1_batches,test_input_batches,test_output1_batches,learning_rate=0.001,momentum=0,dropout_rate=0,epochs=100,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_layer_train_loss_array, two_layer_test_loss_array, two_layer_train_accuracy_array, two_layer_test_accuracy_array = two_layer_single_task_train_nn(train_input,test_input,train_label1_onehot,test_label1_onehot,input_batches,output1_batches,test_input_batches,test_output1_batches,learning_rate=0.001,momentum=0,dropout_rate=0,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "two_task_train_loss_array1, two_task_train_loss_array2, two_task_test_loss_array1, two_task_test_loss_array2, two_task_train_accuracy_array1, two_task_train_accuracy_array2, two_task_test_accuracy_array1, two_task_test_accuracy_array2 = two_layer_two_task_train_nn(train_input,test_input,train_label1_onehot,train_label2_onehot,test_label1_onehot,test_label2_onehot,input_batches,output1_batches,output2_batches,test_input_batches,test_output1_batches,test_output2_batches,learning_rate=0.001,momentum=0,dropout_rate=0,epochs=100)                                                                                                                                                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !! LOSS CURVES !! ##\n",
    "file_name = '../{}.png'\n",
    "\n",
    "######### SINGLE LAYER SINGLE TASK #################\n",
    "plt.plot(np.arange(100),single_layer_train_loss_array, label='Training Loss')\n",
    "plt.plot(np.arange(100),single_layer_test_loss_array, label='Test Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.title('SingleLayerSingleTask: Train and Test Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig(file_name.format('single_layer_loss_task1'))\n",
    "plt.close()\n",
    "\n",
    "########## TWO LAYER SINGLE TASK #################\n",
    "plt.plot(np.arange(100),two_layer_train_loss_array, label='Training Loss')\n",
    "plt.plot(np.arange(100),two_layer_test_loss_array, label='Test Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.title('TwoLayerSingleTask: Train and Test Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig(file_name.format('two_layer_loss_task1'))\n",
    "plt.close()\n",
    "\n",
    "######### TWO LAYER TWO TASK #################\n",
    "\n",
    "plt.plot(np.arange(100),two_task_train_loss_array1, label='Training Loss 1')\n",
    "plt.plot(np.arange(100),two_task_test_loss_array1, label='Test Loss 1')\n",
    "plt.plot(np.arange(100),two_task_train_loss_array2, label='Training Loss 2')\n",
    "plt.plot(np.arange(100),two_task_test_loss_array2, label='Test Loss 2')\n",
    "plt.legend(loc='best')\n",
    "plt.title('TwoLayerTwoTask: Train and Test Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig(file_name.format('two_task_loss_task1'))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "## !! ACCURACY CURVES !! ##\n",
    "\n",
    "######### SINGLE LAYER SINGLE TASK #################\n",
    "plt.plot(np.arange(100),single_layer_train_accuracy_array,label = 'Training accuracy')\n",
    "plt.plot(np.arange(100),single_layer_test_accuracy_array,label = 'Test accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.title('SingleLayerSingleTask: Train and Test Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy') \n",
    "plt.savefig(file_name.format('single_layer_accuracy_task1'))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "########## TWO LAYER SINGLE TASK #################\n",
    "plt.plot(np.arange(100),two_layer_train_accuracy_array,label = 'Training accuracy')\n",
    "plt.plot(np.arange(100),two_layer_test_accuracy_array,label = 'Test accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.title('TwoLayerSingleTask: Train and Test Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy') \n",
    "plt.savefig(file_name.format('two_layer_accuracy_task1'))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "########## TWO LAYER TWO TASK #################\n",
    "plt.plot(np.arange(100),two_task_train_accuracy_array1,label = 'Training accuracy 1')\n",
    "plt.plot(np.arange(100),two_task_test_accuracy_array1,label = 'Test accuracy 1')\n",
    "plt.plot(np.arange(100),two_task_train_accuracy_array2,label = 'Test accuracy 2')\n",
    "plt.plot(np.arange(100),two_task_test_accuracy_array2,label = 'Test accuracy 2')\n",
    "plt.legend(loc='best')\n",
    "plt.title('TwoLayerTwoTask: Train and Test Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy') \n",
    "plt.savefig(file_name.format('two_task_accuracy_task1'))\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
